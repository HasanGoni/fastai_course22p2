"""Reproducing lesson 16"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/02_preprocessing.lesson_16.ipynb.

# %% auto 0
__all__ = ['inplace', 'collate_dict', 'get_dls', 'DataLoaders', 'with_cbs', 'run_cbs', 'CancelBatchException',
           'CancelEpochException', 'CancelFitException', 'BasicCallbackLearner', 'Metric', 'Accuracy', 'DeviceCB',
           'to_cpu']

# %% ../../nbs/02_preprocessing.lesson_16.ipynb 3
from cv_tools.core import *


# %% ../../nbs/02_preprocessing.lesson_16.ipynb 6
def inplace(f):
    def _f(b):
        f(b)
        return b
    return _f

# %% ../../nbs/02_preprocessing.lesson_16.ipynb 9
def collate_dict(ds):
    get = itemgetter(*ds.features)
    def _f(b): return get(default_collate(b))
    return _f

# %% ../../nbs/02_preprocessing.lesson_16.ipynb 10
def get_dls(train_ds, valid_ds, bs, **kwargs):
    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),
            DataLoader(valid_ds, batch_size=bs*2, **kwargs))

# %% ../../nbs/02_preprocessing.lesson_16.ipynb 11
class DataLoaders:
    def __init__(self, *dls): self.train,self.valid = dls[:2]

    @classmethod
    def from_dd(cls, dd, batch_size, as_tuple=True, **kwargs):
        f = collate_dict(dd['train'])
        return cls(*get_dls(*dd.values(), bs=batch_size, collate_fn=f, **kwargs))

# %% ../../nbs/02_preprocessing.lesson_16.ipynb 12
class with_cbs:
	"Decorator to add callbacks to a function."
	def __init__(self, nm): self.nm = nm
	def __call__(self, f):
		"Call the function with the callbacks."
		def _f(o, *args, **kwargs):
			"Try to run the function with the callbacks."
			try:
				# First, we call the callback function with the name 'before_' followed by the name of the function we're decorating.
				# This allows us to execute any necessary setup or preprocessing before the function is called.
				o.callback(f'before_{self.nm}')
				# Next, we call the original function with the provided arguments and keyword arguments.
				# This is where the actual work of the function is done.
				f(o, *args, **kwargs)
				# Finally, we call the callback function again, this time with the name 'after_' followed by the name of the function.
				# This allows us to execute any necessary cleanup or postprocessing after the function has been called.
				o.callback(f'after_{self.nm}')
			except globals()[f'Cancel{self.nm.title()}Exception']: pass
			# This line catches a specific exception that is dynamically generated based on the name of the function being decorated.
			# The exception name is constructed by concatenating 'Cancel' with the capitalized first letter of the function name.
			# If this exception is caught, the code simply passes, effectively ignoring the exception and not propagating it further.
		return _f


# %% ../../nbs/02_preprocessing.lesson_16.ipynb 16
def run_cbs(cbs, method_nm):
	for cb in sorted(cbs, key=attrgetter('order')):
		method = getattr(cb, method_nm, None)
		if method is not None: method()

# %% ../../nbs/02_preprocessing.lesson_16.ipynb 18
class CancelBatchException(Exception): pass
class CancelEpochException(Exception): pass
class CancelFitException(Exception): pass


# %% ../../nbs/02_preprocessing.lesson_16.ipynb 19
class BasicCallbackLearner:
    def __init__(self, model, dls, loss_func, cbs, opt_func=optim.SGD, lr=1e-1):
        store_attr()

        # Here we're looping through each callback in self.cbs and setting its learn attribute to self, 
        # which is the current CallbackLearner instance. This allows each callback to access the learner.
        for cb in self.cbs: cb.learn = self

    def one_batch(self):
        self.preds = self.model(self.batch[0])
        self.loss = self.loss_func(self.preds, self.batch[1])
        if self.model.training:
            self.loss.backward()
            self.opt.step()
            self.opt.zero_grad()

    def one_epoch(self, train):
        self.model.train(train)
        self.dl = self.dls.train if train else self.dls.valid
        try: 
            self.callback('before_epoch')
            for self.iter, self.batch in enumerate(self.dl):
                try:
                    self.callback('before_batch')
                    self.one_batch()
                    self.callback('after_batch')
                except CancelBatchException: pass
            self.callback('after_epoch')
        except CancelEpochException: pass

    def fit(self, n_epochs):
        self.n_epochs = n_epochs
        self.epochs = range(n_epochs)
        self.opt = self.opt_func(self.model.parameters(), self.lr)
        try: 
            self.callback('before_fit')
            for self.epoch in self.epochs:
                self.one_epoch(True)
                with torch.no_grad():
                    self.one_epoch(False)
            self.callback('after_fit')
        except CancelFitException: pass
    
    def callback(self, method_nm):
        run_cbs(self.cbs, method_nm)


# %% ../../nbs/02_preprocessing.lesson_16.ipynb 29
class Metric:
    def __init__(self):
        self.reset()

    def reset(self):
        self.vals, self.ns = [], []

    def add(self, inp, targ=None, n=1):
        self.last = self.calc(inp, targ)
        self.vals.append(self.last)
        self.ns.append(n)

    @property
    def value(self):
        ns = torch.tensor(self.ns)
        return (torch.tensor(self.vals)*ns).sum()/ns.sum()

    def calc(self, inp, targ):
        return inp


# %% ../../nbs/02_preprocessing.lesson_16.ipynb 30
class Accuracy(Metric):
    def calc(self, inp, targ):
        return (inp == targ).float().mean()



# %% ../../nbs/02_preprocessing.lesson_16.ipynb 31
class DeviceCB(CallBack):
    def __init__(self, device=def_device): store_attr()
    def before_fit(self):
        self.learn.model.to(self.device)
    def before_batch(self):
        self.learn.batch = to_device(self.learn.batch)


# %% ../../nbs/02_preprocessing.lesson_16.ipynb 36
def to_cpu(x):
	if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}
	if isinstance(x, list): return [to_cpu(o) for o in x]
	if isinstance(x, tuple): return tuple(to_cpu(o) for o in x)
	return x.detach().cpu()
