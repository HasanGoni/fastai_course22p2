"""Reproducing lesson 17"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/05_preprocessing.lesson_17.ipynb.

# %% auto 0
__all__ = ['def_device', 'device', 'norm', 'clean_ipython_hist', 'clean_tb', 'clean_mem', 'transformi', 'get_model',
           'init_weights', 'BatchTransformCB', 'prep_data_n', 'GeneralRelu', 'plot_func', 'conv_relu', 'get_model_']

# %% ../../nbs/05_preprocessing.lesson_17.ipynb 3
from cv_tools.core import *
from fastcore.all import *


# %% ../../nbs/05_preprocessing.lesson_17.ipynb 4
import sys, traceback, gc
import matplotlib.pyplot as plt
from datasets import load_dataset


# %% ../../nbs/05_preprocessing.lesson_17.ipynb 5
import torch
import torchvision.transforms.functional as TF
import torch.nn as nn
import torch.nn.functional as F
from torcheval.metrics import MulticlassAccuracy
torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)
torch.manual_seed(1)


# %% ../../nbs/05_preprocessing.lesson_17.ipynb 6
import logging
logging.disable(logging.WARNING)

# %% ../../nbs/05_preprocessing.lesson_17.ipynb 7
def clean_ipython_hist():
    # Code in this function mainly copied from IPython source
    if not 'get_ipython' in globals(): return
    ip = get_ipython()
    user_ns = ip.user_ns
    ip.displayhook.flush()
    pc = ip.displayhook.prompt_count + 1
    for n in range(1, pc): user_ns.pop('_i'+repr(n),None)
    user_ns.update(dict(_i='',_ii='',_iii=''))
    hm = ip.history_manager
    hm.input_hist_parsed[:] = [''] * pc
    hm.input_hist_raw[:] = [''] * pc
    hm._i = hm._ii = hm._iii = hm._i00 =  ''

# %% ../../nbs/05_preprocessing.lesson_17.ipynb 8
def clean_tb():
    # h/t Piotr Czapla
    if hasattr(sys, 'last_traceback'):
        traceback.clear_frames(sys.last_traceback)
        delattr(sys, 'last_traceback')
    if hasattr(sys, 'last_type'): delattr(sys, 'last_type')
    if hasattr(sys, 'last_value'): delattr(sys, 'last_value')

# %% ../../nbs/05_preprocessing.lesson_17.ipynb 9
def clean_mem(torch_:bool=True):
    clean_tb()
    clean_ipython_hist()
    gc.collect()
    if torch_: torch.cuda.empty_cache()

# %% ../../nbs/05_preprocessing.lesson_17.ipynb 12
from fastai_course22p2.preprocessing.lesson16_after_lesson import *
from fastai_course22p2.preprocessing.lesson16_second_part import *

# %% ../../nbs/05_preprocessing.lesson_17.ipynb 14
@inplace
def transformi(b): b['image'] = TF.to_tensor(b['image'])


# %% ../../nbs/05_preprocessing.lesson_17.ipynb 17
def_device = 'cuda' if torch.cuda.is_available() else 'cpu'
device = def_device

# %% ../../nbs/05_preprocessing.lesson_17.ipynb 18
def get_model():
	return nn.Sequential(
		conv(1, 8), 
		conv(8, 16), 
		conv(16, 32), 
		conv(32, 64), 
		conv(64,10, act=False), 
		nn.Flatten()).to(device)


# %% ../../nbs/05_preprocessing.lesson_17.ipynb 27
from math import sqrt

# %% ../../nbs/05_preprocessing.lesson_17.ipynb 40
def init_weights(m):
	if isinstance(m, (nn.Conv2d, nn.Conv1d, nn.Conv3d)):
		nn.init.kaiming_normal_(m.weight)
		


# %% ../../nbs/05_preprocessing.lesson_17.ipynb 49
class BatchTransformCB(Callback):
	def __init__(
			self, tfm,
			on_train=True, on_val=True):
		store_attr()

	def before_batch(self, learn):
		if (self.on_train and learn.training) or (self.on_val and not learn.training):
			learn.batch = self.tfm(learn.batch)

# %% ../../nbs/05_preprocessing.lesson_17.ipynb 50
def _norm(batch):
	return (batch[0] - xmean ) / xstd, batch[1]

norm = BatchTransformCB(_norm)

# %% ../../nbs/05_preprocessing.lesson_17.ipynb 56
def prep_data_n():
	dsd = load_dataset('fashion_mnist')
	tds = dsd.with_transform(transformi_n)
	dls = DataLoaders.from_dd(tds, batch_size=1024, num_workers=0)

	return dls


# %% ../../nbs/05_preprocessing.lesson_17.ipynb 60
class GeneralRelu(nn.Module):
	def __init__(self, leak=None, sub=None, maxv=None): 
		super().__init__()
		store_attr()
	def forward(self, x):
		x = F.leaky_relu(x, self.leak) if self.leak else F.relu(x)
		if self.sub: x -= self.sub
		if self.maxv: x = x.clamp_max(self.maxv)
		return x



# %% ../../nbs/05_preprocessing.lesson_17.ipynb 61
def plot_func(f, start=-5, end=5, n=100):
	x = torch.linspace(start, end, n)
	y = f(x)
	plt.plot(x, y)
	plt.grid(True, which='both', ls='--')
	plt.axhline(0, color='k', linewidth=0.7)
	plt.axvline(0, color='k', linewidth=0.7)


# %% ../../nbs/05_preprocessing.lesson_17.ipynb 63
def conv_relu(ni, nf, ks=3, stride=2,act=nn.ReLU ):
	res = nn.Conv2d(
			ni, 
			nf, 
			kernel_size=ks, 
			stride=stride, 
			padding=ks//2)
	if act: res = nn.Sequential(res, act())
	return res



# %% ../../nbs/05_preprocessing.lesson_17.ipynb 64
def get_model_(act=nn.ReLU, nfs=None):
	if nfs is None: nfs = [1, 8, 16, 32, 64]
	layers = [conv_relu(nfs[i], nfs[i+1], act=act) for i in range(len(nfs)-1)]
	return nn.Sequential(*layers, conv_relu(nfs[-1], 10, act=False), nn.Flatten()
					  ).to(def_device)



# %% ../../nbs/05_preprocessing.lesson_17.ipynb 65
def init_weights(m, leaky=None):
	if isinstance(m, (nn.Conv2d, nn.Conv1d, nn.Conv3d)):
		nn.init.kaiming_normal_(m.weight,a=leaky)


